* Android Binder
  CLOSED: [2011-03-18 Fri 11:05]
  - State "DONE"       [2011-03-18 Fri 11:05]
  Android Binder包括三部分:
  - Java Binder
  - C++ Binder
  - Binder driver
** Binder driver
Binder driver 中有四个最重要的数据结构:
- binder_proc
- binder_node
- binder_ref
- binder_thread
*** binder_proc
     binder_proc是binder driver中处于最顶层的数据结构, 它代表了一个与binder
     driver打交道的进程, 任意一个和binder打交道的进程, 不管是server还是client,
     在kernel都有且只有一个binder_proc结构与之对应.

     binder_proc主要成员包括:
     - files
       与进程的task_struct中的files相同, 通过binder传递fd (文件描述符)时需要使用
       该成员
     - VMA
       与进程task_struct中的mm相同, 分配/释放 binder buffer时需要使用该成员
     - nodes
       该进程"拥有"的binder_node, 可以暂且把一个binder_node看作进程对外提供的一
       个service, 所以nodes相当于该进程对外提供的所有service.

     - threads
       进程中所有的binder_thread, 这些binder_thread是一些真正的worker thread,
       binder_node所代表的service就是在这些线程中执行任务的.

     - buffers, free_buffers, allocated_buffers
       每个进程有固定大小的buffer (1M-8K), 用来保存binder调用时的参数和返回值
     - refs_by_desc, refs_by_node
       这两个结构体是两根红黑树, 树结点的value都是binder_ref, 但对于refs_by_desc树,
       结点的key是binder_ref.desc, 对于refs_by_node, key是binder_node

**** binder_proc结构体的初始化
      binder_proc代表一个与binder打交道的进程, 不论binder server或是binder
      client, 都有且仅有一个binder_proc与之对应.
#+BEGIN_HTML
<pre lang="c++" line="1">
      binder_proc的初始化:
      ProccessState::self()
        new ProcessState()
	  open /dev/binder
	  ;; 以下为driver代码
	    binder_open()
	      kmalloc(proc)
	        filp->private_data=proc
</pre>
#+END_HTML
      filp->private_data是进程私有数据, 每当进程通过系统调用进入kernel时, driver
      代码总是可以通过task_struct->filp->private_data轻松找到该进程对应的
      binder_proc
**** binder_proc的清除
      当进程终止时, binder_proc会被driver清除.

      binder_proc的清除:
      当进程终止时 (正常退出或因为信号异常退出), kernel会负责关闭该进程打开的所
      有文件描述符, 因为进程通过打开过/dev/binder, 所以kernel会关闭该文件描述符,
      这个动作会导致driver的binder_release函数被调用:
#+BEGIN_HTML
<pre lang="c++" line="1">
      binder_release()
        binder_proc proc = filp->private_data;
	// service manager进程挂掉了...
	if (binder_context_mgr_node->proc==proc):
	  binder_context_mgr_node=NULL;
	// 释放该进程所有的binder_thread
	// 释放该进程所有binder_node,同时通知使用这些 binder_node (service) 的所
	// 有 binder_ref (client):  这个binder挂掉了 (binder.linkToDeath)
	foreach (node: proc->nodes):
	  // 从 proc->nodes中删除该node
	  rb_erase(node, proc->nodes)
	  // 如果该node没有任何binder_ref使用它 (service没有任何client)
	  if (hlist_empty (node->refs)):
	    kfree(node);
	  else:
	    foreach (ref:node->refs):
	      // linkToDeath
	      if (ref->death):
	        // 给ref->proc->todo添加一个work(通知它node已经dead),
		// 并唤醒 ref 所在的进程
		list_add_tail(ref->death->...)
		wake_up_interruptible(ref->proc->wait);
	// 释放所有的binder_ref
	// 释放所有的binder_buffer
	// 最后释放binder_proc结构体本身
	kfree(proc)
</pre>
#+END_HTML
*** binder_node
     从用户的角度看,binder_node代表一个service, 它与c++ binder的BBinder对
     象有一一对应关系.

     binder_node的主要成员有:
     - binder_proc proc

       表示这个binder_node属于哪个进程. 即这个service是哪个进程提供的.
     - void * ptr
       这个binder_node对应的用户空间的c++ BBinder对象的地址

     - refs
       还记得前面提到的binder_proc被清除时如何处理linkToDeath的么?


     binder_node与binder_proc的关系:
#+BEGIN_EXAMPLE
     		 -+----------------------+
      -+----------+>+  server binder_proc  +-<------------+
       |            +-----------+----------+              |
       |	           -+---------+------------+            |
       |	            |	  rb_root nodes	   |		|
       |	           -+----------------------+		|
       |		         --/  \--			|
       |	               --/	    \--			|
       |	             --/	       \--		|
       |     	    -/		          \-		|
       |    -+------------+    	  -+------------+	|
       |     | rb_node 1	|      	   |  rb_node 2	|	| 
       |    -+------------+    	  -+------------+	|
      -+-----+	 proc	|	   |	proc   -+-------+
            -+------------+    	  -+------------+
#+END_EXAMPLE
*** binder_ref
     从用户的角度看, binder_ref代表一个client, 它与c++ binder的BpBinder一一对应.

     binder_ref的主要成员有:
     - binder_proc * proc
       binder_ref所在的进程 (使用这个client的进程)
     - binder_node * node
       这个binder_ref所指向的binder_node (client对应的service)
     - uint32_t desc

     - rb_node_desc/rb_node_node
       与binder_proc的refs_by_desc/refs_by_node配合, 以便binder_proc可以根据
       desc/node很快的找到desc/node对应的binder_ref

     binder_ref与binder_node实际上指的一个东西,即所谓的"一体两面", binder_node是
     从server的角度来看, 而binder_ref是从client的角度来看.

     binder_ref代表一个client端的proxy, binder_node类似于server端的stub.

     binder_node与binder_ref是`一对多`的关系, 一个进程的某一个binder_node可能有
     多个进程的多个binder_ref引用它, 即多个进程的client使用同一个进程的同一个service.

     binder_proc, binder_node, binder_ref的关系:
#+BEGIN_EXAMPLE
                 -+--------------------+          -+--------------------+                  +--------------------+
       -+-------->+ server binder_proc | 	 	 |client1 binder_proc |			 |client2 binder_proc |
	      |	       -+--------------------+		-+--------------------+			 +--------------------+
	      |	        |    rb_root nodes   |		 |   refs_by_desc     +---+		 |   refs_by_desc     +---+
	      |	       -+---------+----------+		-+--------------------+	  |		 +--------------------+	  |
	      |	       		  |			 |   refs_by_node     +-+ |		 |   refs_by_node     +-+ |
	      |	-+----------------+    	       	       	-+--------------------+ | |		 +--------------------+ | |
	      |	 |					 		    	| |		 		    	| |
	      |	 |					 		    	| |		 		    	| |
	      |	 |    +-----------+			  +----------------+	| |		  +----------------+	| |
	      |	 |    |binder_node|<-----+-----+       	  |  binder_ref	   |	| |		  |  binder_ref	   |	| |
	      |	 |    +-----------+	 |     |	  +----------------+	| |		  +----------------+	| |
	      |	-+--->+	 rb_node  |	 |     |	  |   desc  	   |	| |		  |   desc  	   |	| |
	      |	      +-----------+	 |     |	  +----------------+	| |		  +----------------+	| |
	     -+-------+	  proc 	  |	 |     |	  |  rb_node_desc  |<---+-+		  |  rb_node_desc  |<---+-+
		      +-----------+	 |     |	  +----------------+ 	|		  +----------------+ 	|
			       		 |     |	  |  rb_node_node  |<---+		  |  rb_node_node  |<---+
					 |     |	  +----------------+			  +----------------+
					 |     +----------+  node      	   |	  +---------------+  node      	   |
					 |		  +----------------+      |       	  +----------------+
					 +----------------------------------------+
#+END_EXAMPLE
*** 应用场景举例
The essential concept behind binder is that: proxy & stub can be
transformed between processes, e.g. stub in the server, when transferred to
the client , becomes a proxy, and vice-versa.

As to the binder driver, it refers to the transformation between
binder_node & binder_ref
**** scenario I
Server initiate one BBinder, and deliver it to the client using Intent.
      - Driver know nothing about BBinder until IPCThreadState.transact() is called.
      - IPCThreadState.transact() call ioctl(BINDER_WRITE_READ) to init a transaction
      - driver first search binder_proc->nodes to detect whether the binder_node
        corresponding to the BBinder exist, if not, init a binder_node and add
        it to the `nodes`, this is how binder_node is initiated.
      - find transaction's target_proc (binder_proc), and use
        target_proc->refs_by_node to see whether a corresponding binder_ref
        already exist, if not, create a new binder_ref, this is how binder_ref
        is initiated.
      - the newly-created binder_ref->desc is set to a process-uniq integer,
        which is similar with the auto-increase _id field in database.  and
        binder_ref->node is set to the binder_node.
      - when client get the BpBinder, BpBinder->handler is set to binder_ref->desc, so that
	BpBinder->transact() knows the corresponding binder_ref
**** scenario II
      after the client get the BpBinder, it calls BpBinder->transact()
      - get BpBinder->handle
      - driver search the corresponding binder_ref in the host binder_proc according to `handle` and binder_proc->refs_by_desc
      - the transaction will fail if no binder_ref is found, or else get binder_node from binder_ref->node, and additionally,
	get target binder_proc through binder_node->proc
      - for now, target binder_proc and binder_node have been found, call (BBinder *)(binder_node->cookie)->transact() in binder_proc
*** binder_thread
     Driver can't execute user-mode BBinder in kernel-mode, How does the driver execute BBinder in server process?

     - The Binder server has several `while(1) {}` thread blocked on IOCTL, waiting for client transaction.
     - when the driver need to execute BBinder, it will first put data (BBinder address, function argument) to a place server can reach, then
       wake up the one of the server thread.
     - those server thread are so-called `Binder Thread #1/#2...`
**** Binder Thread initiate
      - IPCThreadState.startThreadPool() can start a binder thread.
      - ProcessState.joinThreadPool() can turn the calling thread to a binder thread.
      - java process have a born binder thread, because onZygoteInit() will call IPCThreadState.startThreadPool() to init a binder thread.
      - native c++ service must call IPCThreadState.startThreadPool() or ProcessState.joinThreadPool() explicitly.
      - if a process has no binder thread, although driver can find the binder_proc and BBinder, but since there is no binder thread,
	driver simply can't wake up any thread to perform the transaction.
**** Calling stack
      Client:
#+BEGIN_EXAMPLE lang=c
      BpBinder::transact()
       	IPCThreadState::transact()
          IPC~::waitForResponse()
             IPC~::talkWithDriver()
               	ioctl(BINDER_WRITE_READ)
                  drv::binder_thread_write()
                     wake_up server thread
                  drv::binder_thread_read()
                      wait...
             case BR_REPLY
#+END_EXAMPLE
      Server:
#+BEGIN_EXAMPLE lang=c
      IPC~::joinThreadPool
       	IPC~::talkWithDriver()
           ioctl(BINDER_WRITE_READ)
                  drv::binder_thread_write()
                  drv::binder_thread_read()
                      wait for client ....
                      now have work.
       	IPC~::executeCommand(BR_TRAN.)
           cookie::transact()
               BnInterface::onTransact()
           IPC~::sendReply()
              IPC~::talkWithDriver()
                 .....
                 wake up client
#+END_EXAMPLE
*** binder_buffer
     - binder_buffer is used during ONE binder transaction to save request(in the target_proc's)  and reply data (in the host_proc's)
       and this buffer is mmap to user-mode directly. so that user-mode BBinder can access binder_buffer directly.
     - every binder_proc has it's own buffer, size limited to 1M-8k, driver will allocate one binder_buffer from the buffer for every transaction.
     - one binder_proc's bind_buffers are organized in rb_tree, every node control a sized buffer.
     - the rb_tree use `best-fit` rule to allocate binder_buffer, and can `merge/split` on demand to reduce external memory fragmentation.

       all buffers are in a continuous memory block:
#+BEGIN_EXAMPLE
	       0                       1K      	       	       	       	       	       	1M-8K
	       +----------------------------------------------------------------+-------+
       	       |   data1: 1K,allocated |   data2: 2k, free |   data3 521k, alloc| .... 	|
       	       +----------------------------------------------------------------+-------+
#+END_EXAMPLE
       binder_buffer(s) are organized by buffers/free_buffers/allocated_buffers in rb_tree
#+BEGIN_EXAMPLE
	       	       	       	       	       	     +-----------------+
	       					     |	binder_proc    |
	       					     +-----------------+
	       		   +-------------------------+ 	buffers        |
			   |			     +-----------------+
	       		   |  +----------------------+ free_buffers    |
	       		   |  |			     +-----------------+
	       		   |  |			     |allocated_buffers+------------------------+
	       		   |  |			     +-----------------+			|
			   |  |					   				|
			   |  |					   				|
			   |  |   +---------------------+             +---------------------+	|
			   |  |   |  binder_buffer     	|	      |  binder_buffer      |	|
			   |  |   +---------------------+	      +---------------------+	|
			   |  +-->+rb_node(free or not)	|             |rb_node(free or not) |<--+
			   |   	  +---------------------+	      +---------------------+
			   +----->+   list_head entry  	+------------>|   list_head entry   |
				  +---------------------+	      +---------------------+
				  |   data_size	       	|	      |   data_size	    |
				  +---------------------+	      +---------------------+
				  |   data[0]		|	      |   data[0]	    |
				  +---------------------+	      +---------------------+
#+END_EXAMPLE
      client data are mmap to server's binder_buffer.
#+BEGIN_EXAMPLE
 		       User mode   +-----------------+                                 +-----------------+
		       	     	   |   process A     |			   	       |   process B     |
			     	   +-----------------+	    copy_from_user()	       +-----------------+
			     	   |   Parcel data   +------------------------+	       |       	         |
			     	   +-----------------+                        |	       +-------------^---+
									      |			     |
		      --------------------------------------------------------+----------------------+---------------
				  	 binder driver			      |			     |
		       Kernel mode	 +------------------------------------+----------------------+-------------+
					 |				      V		  	     |		   |
					 |     +----------------+          +--+-------------+	     |		   |
				       	 |     | binder_proc B 	|   +----->+ binder_buffer  +--------+		   |
 	       	       	       	       	 |     +----------------+   |  	   +----------------+	mmap to B process  |
					 |     |  allocated_buf	+---+  	   |  parcel data   |			   |
					 |     |     	       	|      	   |  from A   	    |			   |
					 |     |       	       	|      	   +----------------+			   |
					 |     +----------------+						   |
					 +-------------------------------------------------------------------------+
#+END_EXAMPLE
**** binder_buffer de-allocation
      binder_buffer is de-allocated only when user called Parcel::freeData()
      firstly, drv will remove the binder_buffer from
      curr_proc->allocated_buffers.  then, if the buffered's next or prev buffer
      is also free, they will be merged to one larger binder_buffer, and then
      added to the curr_proc->free_buffers.
***** Parcle.freeData()
       free Parcel object ASAP!
       Client should free reply parcel ASAP!
       Server should free data parcel ASAP!
#+BEGIN_EXAMPLE
       Parcel::freeData()
          Parecel::mOwner()
            ioctl(BINDER_WRITE_READ) with BC_FREE_BUFFER
             drv::binder_thread_write()
                 free buffer of the proc
#+END_EXAMPLE
       Parcel::mOwner is actually a callback fun, it is registered to the parcel
       in BC_TRANSACTION (server got the data parcel) or BC_REPLY (client got
       the reply parcel).  The callback will ioctl to the driver the free the
       parcel.

*** binder_transaction_data
     :PROPERTIES:
     :CUSTOM_ID: @binder_transaction_data
     :END:
     binder_transaction_data stores request/reply data, and in most time, is bitwise copied to binder_buffer, but there are several exceptions:
     - Binder
       The BBinder wrote in binder_transaction_data is transformed to BpBinder and vice-versa
     - file descriptor
       new file descriptor is created in the target process, and old file descriptor is transformed to the newly created one.
#+BEGIN_EXAMPLE

       *binder_transaction_data*
       -+-------------+------+-----------+------+----------+-------------+---------+-----------+-----------+-----------+
       	|   target    |	     |   code  	 |	|	   |	       	 |	   |	       |	   |	       |
       	| (handle/ptr)|cookie| (command) | flags|sender_pid| sender_euid |data_size|offset_size| buffer_ptr|offset_ptr |
       -+-------------+------+-----------+------+----------+-------------+---------+-----------+----+------+----+------+
			  								       	    |           |
	-+------------------------------------------------------------------------------------------+	 -+-----+
	 | -+---------+----+--------------+--------+-----+-----+------+----------------			  v
	 |  | type    |flag| binder/handle| cookie | ....|type | flag | ...				 -+-----+--+-------+--------------
  ->--+(binder, |	   |   	       	  |    	   |   	 |     |      |	       	       	       	       	  | offset1|offset2| ...     	
	    | handler,|	   |		  |	   |	 |     |      |					 -+---+----+---+---+--------------
	    | fd..)   |	   |   	       	  |    	   |   	 |     |      |					      |	       |
	    ^---------+----+--------------+--------+-----^-----+------+---------------			      |	       |
	    |    *flag_binder_object*			 |						      |	       |
	   -+--------------------------------------------+----------------------------------------------------+	       |
							-+-------------------------------------------------------------+


#+END_EXAMPLE
**** overall calling sequence
      |-------------------------------------+-------------------------------------------+--------------------------------------|
      | client                              | driver                                    | server                               |
      |-------------------------------------+-------------------------------------------+--------------------------------------|
      | onTransact() ->                     |                                           |                                      |
      | create user_mode tr for parcel data |                                           |                                      |
      |                                     | client:    step 1                         |                                      |
      |                                     | binder_transaction_data tr;               |                                      |
      |                                     | copy_from_user(tr)                        |                                      |
      |                                     | buf=binder_alloc_buf(server,tr.data.size) |                                      |
      |                                     | binder_transaction t;                     |                                      |
      |                                     | t.data=buf                                |                                      |
      |                                     | create binder_work from t                 |                                      |
      |                                     | add binder_work to server's stack         |                                      |
      |                                     | wake up server                            |                                      |
      |                                     |                                           |                                      |
      |                                     | server:    step 2                         |                                      |
      |                                     | get binder_work from stack                |                                      |
      |                                     | get binder_transaction t                  |                                      |
      |                                     | binder_transaction_data tr;               |                                      |
      |                                     | tr.data=t.data                            |                                      |
      |                                     | copy_to_user(tr)                          |                                      |
      |                                     |                                           | get parcel data from tr              |
      |                                     |                                           | create user_mode tr for parcel reply |
      |                                     | server:                                   |                                      |
      |                                     | binder_transaction_data tr;               |                                      |
      |                                     | copy_from_user(tr)                        |                                      |
      |                                     | buf=binder_alloc_buf(client,tr.data.size) |                                      |
      |                                     | ...                                       |                                      |
      |                                     | repeat step 1 and step 2                  |                                      |
      | onTransact() <-                     |                                           |                                      |
      |-------------------------------------+-------------------------------------------+--------------------------------------|

*** binder_transaction & binder_work
     - binder_thread_write() & binder_thread_read() communicate with binder_work & binder_transaction.
     - binder_work are entry added to thread->todo / proc->todo, when server/client are waken up during binder_thread_read(), kernel code will
     - check thread->todo to get a binder_work to do, and extract binder_transaction from binder_work by container_of (binder_work) macro.
     - binder_transaction contains info about the work's caller and receiver and
       the transaction data, then construct a binder_transaction_data according to binder_transaction's data (target binder ptr, cookie,
       sender_uid, parcel data ..), then binder_thread_read() returns to user space code.

     To summarize:

     binder_thread->transaction_stack is the `transaction stack` while binder_transaction is the `transaction stack frame`,
     binder_transaction mainly saves the caller's return info.

     e.g. Client's binder transaction request will be encapsulated to a binder_transaction, and put into target's `transaction_stack`, when target need to
     reply to client, it can get the former binder_transaction from it's own `transaction_stack`, to know who will be waken up for the reply.

seq graph:
#+BEGIN_EXAMPLE
|-----------------------+--------------------------------------------------------+--------------------------------------------------+--------------|
| client                | binder_thread_write                                    | binder_thread_read                               | server       |                       |                                                        |                                                  |              |
|-----------------------+--------------------------------------------------------+--------------------------------------------------+--------------|
| onTransact() ->       | get binder_transaction_data from user mode             |                                                  |              |
|                       | cmd=BC_TRANSACTION                                     |                                                  |              |
|                       | construct binder_transaction and                       |                                                  |              |
|                       | put it to target's transaction_stack                   |                                                  |              |
|                       | add binder_work to target thread->todo                 |                                                  |              |
|                       | wakup target thread                                    |                                                  |              |
|                       |                                                        | get binder_work from thread->todo                |              |
|                       |                                                        | get binder_transaction from                      |              |
|                       |                                                        | binder_work                                      |              |
|                       |                                                        | construct binder_transaction_data for user mode  |              |
|                       |                                                        | reset binder_transaction_data (reset binder,fd.. |              |
|                       |                                                        | and set cmd=BR_TRANSACTION                       |              |
|                       |                                                        |                                                  | onTransact() |
|                       | get binder_transaction_data from user mode             |                                                  |              |
|                       | cmd=BC_REPLY                                           |                                                  |              |
|                       | get binder_transaction from it's own transaction_stack |                                                  |              |
|                       | so as to get target_thread                             |                                                  |              |
|                       | construct binder_transacton and add binder_work to     |                                                  |              |
|                       | target_thread->todo, then wake up target thread        |                                                  |              |
|                       |                                                        | get binder_work from thread->todo                |              |
|                       |                                                        | get binder_transaction                           |              |
|                       |                                                        | construct binder_transaction_data for user mode  |              |
|                       |                                                        | reset binder_transaction_data                    |              |
|                       |                                                        | and set cmd=BR_REPLY                             |              |
| onTransact() waken up |                                                        |                                                  |              |
|                       |                                                        |                                                  |              |
|                       |                                                        |                                                  |              |
|                       |                                                        |                                                  |              |
|-----------------------+--------------------------------------------------------+--------------------------------------------------+--------------|
#+END_EXAMPLE

*** DONE [#C] binder reference count
     CLOSED: [2011-03-18 Fri 17:57]
     - State "DONE"       [2011-03-18 Fri 17:57]
    Binder reference 分为两个方法:
    　- C++ 层面的RefBase
    　- driver 层面 binder_ref 的引用计数
**** When Java Binder is finalized
#+BEGIN_HTML
<pre lang="c++" line="1">
      Binder.finalize() @ Binder.java
        android_os_Binder_destroy() //jni
	  JavaBBinderHolder->decStrong();
	    c = android_atomic_dec(&refs->mStrong)
	    if (c == 1):
              const_cast<RefBase*>(this)->onLastStrongRef(id)
	        {} // nop for BBinder
</pre>
#+END_HTML
**** When Java BinderProxy is finalized
#+BEGIN_HTML
<pre lang="c++" line="1">
      BinderProxy.finalize() @ Binder.java
        android_os_BinderProxy_destroy(JNIEnv* env, jobject obj)
	  BinderProxy.decStrong()
	    c = android_atomic_dec(&refs->mStrong)
	    if (c == 1):
              const_cast<RefBase*>(this)->onLastStrongRef(id)
	        IPCThreadState->decStrongHandle(mHandle);
		  mOut.writeInt32(BC_RELEASE)
                    binder_thread_write()
	              case BC_RELEASE:
	                binder_dec_ref(ref, 1);
	                  ref->strong--;
	                  if ref->strong == 0:
	                   binder_dec_node(ref->node, strong, 1);
	                 if ref->strong == 0 && ref->weak == 0:
	                   binder_delete_ref(ref);
</pre>
#+END_HTML
     What's more:
     - binder_dec_ref() will also be called during binder_free_buffer() and the buffer contains BINDER_TYPE_HANDLE.
     - binder_node will be deleted when:
       binder_proc died or there is no binder_ref of the binder_node
*** DONE [#A] binder's death ( linkToDeath )
     SCHEDULED: <2011-03-01 Tue> CLOSED: [2011-02-28 Mon 13:13]
     - State "DONE"       [2011-02-28 Mon 13:13]

     see [[@AppDeathRecipient]]
     see also [[Android Process Crash and Restart]]

     java: BinderProxy.linkToDeath(DeathRecipient)
     c++   BpBinder.linkToDeath(DeathRecipient)

     - binder dead
       typically because binder fd is closed, e.g. when remote process exit, or user manually called IPCThreadState.stopProcess

     - how DeathRecipient is called
#+BEGIN_EXAMPLE
       close(binder_fd)
         driver::binder_release()
	   driver::binder_deferred_release()
	     foreach node->refs:
	       ref->death->work.type = BINDER_WORK_DEAD_BINDER;
  	       list_add_tail(&ref->death->work.entry, &ref->proc->todo);
	       wake_up_interruptible(&ref->proc->wait);
#+END_EXAMPLE
       when proxy side's binder_thread is waken up, it will read one BR_DEAD_BINDER command, which will execute:
#+BEGIN_EXAMPLE
       BpBinder *proxy = (BpBinder*)mIn.readInt32();
       proxy->sendObituary();
         foreach ob in mObituaries:
	   ob.recipient.binderDead()
#+END_EXAMPLE
     - how DeathRecipient is registered
#+BEGIN_EXAMPLE
       BpBinder.linkToDeath
         Obituary ob;
	 ob.recipient = recipient;
	 IPCTheadState::requestDeathNotification(mHandle, BpBinder);
	   mOut.writeInt32(BC_REQUEST_DEATH_NOTIFICATION);
	   mOut.writeInt32((int32_t)handle);
	   mOut.writeInt32((int32_t)BpBinder); ;;BpBinder's local address is written to driver as *cookie*
	     driver::BC_REQUEST_DEATH_NOTIFICATION
	       ref->death->cookie=*cookie*; ;;BpBinder's local address
         mObituaries->add(ob);
#+END_EXAMPLE
     - If there is no binder_thread running in proxy process, maybe DeathRecipient will never be notified automatically.

*** DONE binder & exception
     CLOSED: [2011-03-10 Thu 13:50]
     - State "DONE"       [2011-03-10 Thu 13:50]
     - State "DONE"       [2011-02-22 Tue 19:08]
**** remote exceptions
      `remote exceptions` stands for `exceptions occurred during stub.onTransact()`
***** proxy side
      - binder.stub.proxy
#+BEGIN_EXAMPLE
  	public int foo() throws android.os.RemoteException {
	     android.os.Parcel _data = android.os.Parcel.obtain();
	     android.os.Parcel _reply = android.os.Parcel.obtain();
	     int _result;
	     try {
		 _data.writeInterfaceToken(DESCRIPTOR);
		 mRemote.transact(Stub.TRANSACTION_foo, _data, _reply, 0);
		 _reply.readException();
		 _result = _reply.readInt();
	     } // note: *without `catch clause`*
	     finally {
		 _reply.recycle();
		 _data.recycle();
	     }
	     return _result;
        }
#+END_EXAMPLE
      - _reply.readException()
#+BEGIN_EXAMPLE
	return if code==0
       	switch (code) {
             case EX_SECURITY:
                 throw new SecurityException(msg);
             case EX_BAD_PARCELABLE:
                 throw new BadParcelableException(msg);
             case EX_ILLEGAL_ARGUMENT:
                 throw new IllegalArgumentException(msg);
             case EX_NULL_POINTER:
                 throw new NullPointerException(msg);
             case EX_ILLEGAL_STATE:
                 throw new IllegalStateException(msg);
         }
#+END_EXAMPLE
***** stub side
      - Binder.execTransact()
#+BEGIN_EXAMPLE lang=c
        try {
            res = onTransact(code, data, reply, flags);
	    // if exceptions other than RemoteException and RuntimeException are thrown here,
	    // the outer JavaBBinder jni wrapper class will handle it, and output message like
	    // "Uncaught remote exception!
	    // Exceptions are not yet supported across processes"
        } catch (RemoteException e) {
            reply.writeException(e);
            res = true;
        } catch (RuntimeException e) {
            reply.writeException(e);
            res = true;
        }
#+END_EXAMPLE
      - writeException()
#+BEGIN_EXAMPLE lang=c
	int code = 0;
        if (e instanceof SecurityException) {
            code = EX_SECURITY;
        } else if (e instanceof BadParcelableException) {
            code = EX_BAD_PARCELABLE;
        } else if (e instanceof IllegalArgumentException) {
            code = EX_ILLEGAL_ARGUMENT;
        } else if (e instanceof NullPointerException) {
            code = EX_NULL_POINTER;
        } else if (e instanceof IllegalStateException) {
            code = EX_ILLEGAL_STATE;
        }
        writeInt(code);
	if (code==0) throw new RuntimeException();
#+END_EXAMPLE
      - Binder.stub.onTransact()
#+BEGIN_EXAMPLE lang=c
	case TRANSACTION_foo:
	{
	    data.enforceInterface(DESCRIPTOR);
	    int _result = this.foo();
	    reply.writeNoException();
	    reply.writeInt(_result);
	    return true;
	}
#+END_EXAMPLE
**** local exceptions
      `local exceptions` stands for `exceptions occurred other than stub.onTransact(), e.g. binder error`
#+BEGIN_EXAMPLE
      try {
        _data.writeInterfaceToken(DESCRIPTOR);
	mRemote.transact(Stub.TRANSACTION_foo, _data, _reply, 0); // binder's own exceptions may be thrown here .
	  BinderPorxy.transact()
	    android_os_BinderProxy_transact() // native here
	      err=BpBinder.transact()
	        err=IPCTheadState.transact()
		  err=waitForResponse()
		    .. talk with driver, may return err for errors like FAILED_EXCEPTION, DEAD_OBJECT, NO_MEMORY, PERMISSION_DENIED ...
	      signalExceptionForError(err);
	        switch err:
		  case EPERM: jniThrowException("java/lang/SecurityException")
		  case FAILED_TRANSACTION: LOGE("!!! FAILED BINDER TRANSACTION !!!"); // doesn't throw exceptions!
		  ...
	_reply.readException(); // stub initiated exceptions may be thrown here
	_result = _reply.readInt();
      }
#+END_EXAMPLE
**** To summarize
      - binder.stub.proxy need to invoke Parcel.readException() to manually detect whether exceptions occurred in the stub side.
      - only 5 RuntimeException occurred in stub.onTransact can be eventually caught by stub and send to proxy (RemoteException is caught by execTransact,
	but not wrote to reply...weired~)
      - proxy side's readException only throws 5 RuntimeException
      - RemoteException happened in stub side will not be caught by proxy side.
	RemoteException should only be thrown by proxy's own code, stub's code shouldn't throw any exceptions other than those 5 RuntimeException
#+BEGIN_EXAMPLE
					   ____   ___   ___  __  __ _
					  | __ ) / _ \ / _ \|  \/  | |
      					  |  _ \| | | | | | | |\/| | |
					  | |_) | |_| | |_| | |  | |_|
					  |____/ \___/ \___/|_|  |_(_)
							^
							|
							|     2.remote exceptions popped
							|	up through binder reply
		 		  			|   .................................
		 		  			|   .  	                       	    .
		 		       	       	       	|   .  	       	      -+------------+-----------------+
		 		       	      proxy	|   v	    	       |       	       	    stub      |
       	       	       	       	       	     -+---------+------------+ 	       |  -+--------------------+     |
		 		    +-------->|	       	       	     | 	       |   |   	       	       	|     |
		 		    |	      |	  java 	transact()   |	       |   |   java transact()	|     |
		 	       	    |	-+----+----------------------+---+     |   |	   	      	|     |
	       1.local exceptions   |	 |   -+-------jni------------+ 	 |     |  -+--------------------+     |
       	       	 popped up through  |	 |    |	      	    	     |	 |    -+---+--------------------+-----+
		 function ret code  |	 |    |	  c++  	transact()   |	 |     	   |   c++ transact()	|
				    |	 |    |		    	     |	 |     	   |	   	     	|
				   -+----+   -+-------ioctl----------+---+---------+-------ioctl--------+---+
				     	 |    |	       	    	      	 |		             	    |
				     	 |    |	       	       	       	 |		               	    |
       	       	       	       	     	 |    |		 	      	 |		                    |
       	       	       	       	       	 |    |		 	      	 |		                    |
					 |    |		        kernel	 |		                    |
       	       	       	       	       	 |   -+--------------------------+----------------------------------+
     					-+-------------------------------+

#+END_EXAMPLE

*** [#B] binder wait & todo
** C++
*** BpInterface
12/9/2010
BpInterface:IInterface
BpInterface resides in the proxy part, but it't nothing but an encapsulation of the remote IBinder (actually BpBinder)

it's most import method is BpInterface.asInterface(IBinder)
, it will implement those IXX.xx functions and dispatch them to BpBinder.transact().

*** IInterface
*** BnInterface
12/9/2010
BnInterface:BBinder,IInterface
so, BnInterface resides in the stub part, and mainly act as two parts:
1) it extends IInterface, so it will implement IXX.xx functions
2) it extends BBinder, so it will implement onTransact(), it will be called by BBinder.transact(), and will dispatch transact code to it's own IXX.xx functions.

*** BBinder
12/9/2010
BBinder corresponds to BpBinder, it is the real stub part.

when IPCThreadState discovers an IPC, it will call BBinder.transact(), which will call deprived class's (which is usually BnInterface) onTransact()

*** BpBinder
12/9/2010
when BnInterface, as the stub part,is returned to client as IBinder through Parcel.writeStrongBinder and Parcel.readStrongBinder, the proxy part actually will get a BpBinder as IBinder (since both BpBinder and BBinder extends IBinder).

To make it clear, BpInterface use BpBinder, and BnInterface extends BBinder

BpBinder's most import method is transact(), which will call IPCThreadState.transact() to interact will binder driver.

BpBinder's member variable `handle` could be used by the driver to distinguish it from other BpBinder and found the corresponding strub process.
*** Binder Thread
*** ProcessState
12/9/2010
both the proxy and stub process has one and only one ProcessState.
1) proxy part:
BpBinder->transact()->IPCThreadState.transact()

IPCThreadState need to use ProcessState to interact with binder driver

2) stub part:

**** StartThreadPool
*** IPCThreadState
**** JoinThreadPool
** Java/AIDL
*** IXx.stub
     stub is essentially a binder, but it do additional two things:
     1. Imlements some stubs side IXx funtions.
     2. Auto dispatching to those functions according to unmarshalling result.


     stub implement both Binder and IXx.
     1. It implements Binder, and  implements onTransact(), so it can be returned when OnBind, or addService as IBinder... as the stub.
     2. It implements IXx, so it will implents those stub side functions of IXx,
        e.g.  IXx.foo.  stub.onTransact will call IXx.foo according to the
        unmarshalling result.

*** IXx.stub.proxy
     proxy only implements IXx, and it will take an IBinder as ctor param.  It should be taken as a helper utility for the ibinder.
     So:
     1. It implements IXx, so it will implements IXx proxy side functions,
        e.g. IXx.foo, those functions do nothing but marshalling the params and
        then call remote->transact.
     2. It take an IBinder as ctor param(the 'remote' var), so it can use the
        IBinder to do proxy works. e.g. Call ibinder->transact.
     IXx.asInterface(ibinder) actually call IXx.stub.proxy(ibinder) to convert the ibinder as an proxy interface.
** Java Binder vs. C++ binder
    java binder is just an encapsulation of c++ binder
    Binder,BinderProxy vs. JavaBBinder, BpBinder
*** binder proxy in java
#+BEGIN_EXAMPLE
     IBinder.transact()   // binder proxy in java is BinderProxy
       BinderProxy.transact() // native
         android_os_BinderProxy_transact() // in android_util_Binder.cpp
	   IBinder* target = env->GetIntField(obj, gBinderProxyOffsets.mObject); //target is a BpBinder
	     err=BpBinder->transact()->IPCThreadState->transact()
	       ...
	     signalExceptionForError(env, obj, err);
#+END_EXAMPLE
*** binder stub in java
     - initialization
#+BEGIN_EXAMPLE
       Binder() // ctor
         init(); // native
           android_os_Binder_init()
	     JavaBBinderHolder(env, clazz); // JavaBBinderHolder is a lazy holder for JavaBBinder
	     // lazy evaluation of JavaBBinderHolder.get() will init JavaBBinder and set to gBinderOffsets.mObject
	     env->SetIntField(clazz, gBinderOffsets.mObject, (int)jbh);
#+END_EXAMPLE
     - writeStrongBinder
#+BEGIN_EXAMPLE
       Parcel.writeStrongBinder() // native
         android_os_Parcel_writeStrongBinder()
	   parcel->writeStrongBinder(ibinderForJavaObject(env, object));
	     if (env->IsInstanceOf(obj, gBinderProxyOffsets.mClass)): // binder is proxy
	       env->GetIntField(obj, gBinderProxyOffsets.mObject);
	     else if (env->IsInstanceOf(obj, gBinderOffsets.mClass)): // binder is stub
	       env->GetIntField(obj, gBinderOffsets.mObject); // binder initialization will set this field
#+END_EXAMPLE
     - onTransact
#+BEGIN_EXAMPLE
       JavaBBinder.onTransact() // c++
         env->CallBooleanMethod(mObject, gBinderOffsets.mExecTransact, ...)
	   Binder.execTransact()
	     Binder.onTransact()
         excep = env->ExceptionOccurred();
	 report_exception(env, excep,...)
#+END_EXAMPLE

** misc
*** Parcelable VS. Serializable
     Parcelable:
     fast and lightweight.  But there are some limitations:
     1. all the Container class is NOT parcelable but serializable, so u can't call
       	Intent.putExtra(List..).  Although u can use intent.putParcelableArrayList(),
       	the items in the list must be Parcelable.....
     2. Bundle internally is a Map<String,Object>, and calls Parcel.writeValue(object)  recursively to write anything,  the function looks like:
       	if (obj instance of String) ...
       	if (obj instance of Map) ..
       	if (obj instance of List) ...
       	....
       	if (obj instance of Parcelable)

       	so u see, Parcelable is the last resort, so if you rewrite Map which
       	implements Parcleble, it will NEVER be taken as a Parcelable by Bundle,
       	since it's firstly a Map...

     To summurize it: complex Containers could NOT be transmitted through parcel.

     serializable is java's game. It's powerful but quite SLOOOOW.
     u can serialize any complex class, just DECLARE it implements 'Serializable'.  Also u can optionally provid
*** IBinder as token
*** ParcelFileDescriptor
*** DONE Binder.getCallingPid() & Binder.getCallingUid()
     CLOSED: [2011-02-22 Tue 17:23]
     - State "DONE"       [2011-02-22 Tue 17:23]
     see [[@Android Permission]]
     see [[@binder_transaction_data]]
     - these two static functions can be used to retrieve the calling pid/uid of the binder proxy thread, but if the current thread is
       not executing an incoming binder transaction, then it's own pid/uid is returned.
     - these functions can be used by the binder_stub to detect whether the binder_proxy has appropriate permissions, through
       `context.checkPermission(permission,calling_pid,calling_uid)`, or it's analog `context.checkSelfOrCallingPermission(permission)`

     Binder.getCallingPid()
       IPCThreadState.getCallPid()
         return mCallingPid

     IPCThreadState.waitForResponse()
       ;; for binder stub, block to get transaction request
       executeCommand();
         ;; BR_TRANSACTION
         get `binder_transaction_data` from binder_buffer,
         orig_pid=mCallingPid        ;; save orig pid
         mCallingPid=tr.sender_pid   ;; get sender pid from binder_transaction_data
         (*cookie)->transact()       ;;
         mCallingPid=orig_pid        ;; restore orig pid

    mCallingPid is initialized to getPid() by IPCThreadState, so that Binder.getCallPid() may return thread's own pid if not executing an incoming
    binder transaction.
*** Binder.clearCallingIdentity() & Binder.restoreCallingIdentity()
during one binder transaction, if the binder-stub want to temporary use it's own pid/uid (instead of proxy's pid/uid) for permission-check, it
should invoke clearCallingIdentity to temporary save proxy's pid/uid to one `long int`, then set getCallingPid to stub's own pid.
After the permission-check, it could invoke restoreCallingIdentity(long) to restore getCallingPid to proxy's pid.
*** AsyncTask
*** Messenger
     :PROPERTIES:
     :CUSTOM_ID: @Messenger
     :END:
     handy class which provide a handler within a binder, so that message can be sent across processes.
